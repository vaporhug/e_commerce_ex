{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# 数据检查",
   "id": "b050829591bfdc2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-27T00:18:43.732035Z",
     "start_time": "2024-10-27T00:18:10.842811Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# 定义文件夹路径\n",
    "folder_path = 'data'\n",
    "\n",
    "def check_file_content_by_dir(file_path):\n",
    "    for root,dirs,files in os.walk(file_path):\n",
    "        for file in files:\n",
    "            if  file.endswith(('.txt','.zip')):\n",
    "                continue\n",
    "            print(f\"File: {os.path.join(root,file)}\")\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(root,file),encoding='gb18030',on_bad_lines='skip',header=None)\n",
    "                \n",
    "                \n",
    "                # 打印第一条数据\n",
    "                if not df.empty:\n",
    "                    print(\"First row:\", df.iloc[0].to_dict())\n",
    "                else:\n",
    "                    print(\"This file is empty.\")\n",
    "                \n",
    "                print(\"-\" * 40)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not read {file}: {e}\")\n",
    "\n",
    "check_file_content_by_dir('data')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: data/.DS_Store\n",
      "Could not read .DS_Store: 'gb18030' codec can't decode byte 0x80 in position 3131: illegal multibyte sequence\n",
      "File: data/sougou_competition_data/user_tag_query.10W.TEST\n",
      "Could not read user_tag_query.10W.TEST: 'gb18030' codec can't decode byte 0x80 in position 65767: illegal multibyte sequence\n",
      "File: data/sougou_competition_data/user_tag_query.10W.TRAIN\n",
      "Header: [0, 1, 2, 3]\n",
      "First row: {0: '22DD920316420BE2DF8D6EE651BA174B\\t1\\t1\\t4\\t柔和双沟\\t女生\\t中财网首页 财经\\thttp://pan.baidu.com/s/1plpjtn9\\t周公解梦大全查询2345\\t曹云金再讽郭德纲\\t总裁大人行行好\\t中财网第一财经传媒\\t教师节全文\\t男子砸毁15墓碑\\t黄岩岛最新填海图\\t引起的疲\\t缘来未迟落跑甜心不好惹\\t梁朝伟与替身同框\\t笑傲江湖电视剧任贤齐\\t小起名字女孩名字\\t海运行李到堪培拉\\t确定\\t诱爱99天 司少的天价宝贝\\t什么是遥控魔棒\\t徽信表情动态搞笑图片\\t教师节征文100字\\t安微联通网上营业厅\\t甜宠百分百:校草的萌萌未婚妻\\t豪门重生之暖爱成婚\\tnikehypershift和kd5哪个好看\\t韭菜炒鸡蛋\\t陈赫玩王者荣耀\\t虎牙楚河\\t三国演义小说txt下载\\t威县欧派\\t炒馍花怎么做好吃\\t黄岩岛最新消息2016年\\t中秋节诗句大全祝福\\t教师节征文\\t菜谱\\t柔和双沟卖的怎么样\\t七位数开奖结果\\t以色列停车场坍塌\\t天龙家庭农场\\t7.22什么星座\\t新旧约圣经和合本下载\\t4π\\twifi万能钥匙\\t威灵仙图片\\t临泉长官天龙家庭农场\\t早安总统大人\\t百合\\t莲藕的做法\\t花街\\t无锡\\t蚬壳胃散怎么吃\\t触手忆寒\\t中秋节的诗句\\t孟州电信 电子发票\\t鸡丝汤的做法\\t我等你\\t临泉长官镇桥口李小刚农场\\t朋仇\\t全民k歌\\t炸葱花\\t蒜苔炒肉\\t冰川的图片\\tkd5\\t…\\t若风\\t好奇纸尿裤\\t清蒸鱼\\t189.8是谁的平方\\t重庆餐馆发生爆炸\\t捡手机被失主抢劫\\thttps://yunpan.cn/ocsqfgtfya2ewj\\t炒馍花的家常做法\\t三国演义小说百度云\\t总裁掠爱小舅别太坏\\t:https://yunpan.cn/cmh8tmeyraiww\\t周公解梦\\t查坦克冰川\\t凉拌藕片的做法\\t投票\\t鸡丝炒什么好吃\\t被时光掩埋的秘密小说下载\\t中国电信电子发票\\t张续豪\\t关于月亮的诗句\\t用酵母蒸馒头的方法\\t赵丽颖碧瑶坐\\t触手兵长\\t图集 下载腾讯新闻', 1: '看街头混战武警\\t厦门航空\\t蚬壳胃散\\t炒茄子做法\\t身份类别怎么填\\t最好的我们里面的方特在哪里\\t牢里面的生活是怎样的\\t强迫症有哪些表现\\t白袍法师暖暖图片\\t朋仇广场舞\\t小宇热游\\t蒸馒头的方法\\t狡滑的意思\\t黄石大冶东岳派出所服务电话\\t三国演义小说下载txt\\thttp://zxjhjc9088.1688.com\\t松柏道馆\\t10.1高速免费几天\\t三国演义小说txt\\t柔和双沟业务待遇\\t酵母蒸馒头的方法\\t初中家教一对一辅导\\t口子窖\\t中秋节祝福诗句\\t侠岚\\t文王国窖42度价格表1001文王国窖42度价格表\\t批注是什么意思\\t殿下专属小丫头\\t无锡爆炸\\t炸茄子做法\\thttp://pan.baidu.com/s/1cor7gy\\t大件行李邮寄\\t烟火陈翔\\t没想到', 2: '真没想到作文\\t安徽滁州石坝镇\\t虎牙小宇\\t驾校培训跑长途\\t骨质性关节炎\\t左膝盖内侧疼是怎么回事\\t虎皮尖椒的做法大全\\t陈翔的女朋友吻照\\tq我的世界、5\\t23.04的平方根\\t神将世界表情包\\t寻找成龙\\t柔和双沟卖的\\t三国演义txt百度云\\t一般现在时\\t澳洲邮寄行李费用\\t触手若月\\t1991年11月26日是什么星座\\t校草成长记\\t暮光女向女友求婚\\t钢弩的价格图片\\t乐乐课堂\\t宠妻成瘾老婆你要乖\\t魔手tv\\t梅河口到济南的火车票\\t临泉长官镇\\t君子兰\\t南洋十大邪术电影\\t肚\\t炸油条的做法和配方\\t根号6等于多少\\t笑笑昨日帝王骗\\t吃惊的什么填词语\\t50字教师节征文\\t朝阳区黑庄户邮编\\t千百鲁\\t1991年农历11月26日是什么星座\\t圣经和合本免费下载\\t水煮花生米的做法\\thttp://pan.baidu.com/s/1jhbv9pg\\t十字弓\\t徽信表情含义\\t天才小熊猫微博长图\\t宠冠六宫:帝王的娇蛮皇妃\\t去广告软件 安卓版\\t萌妻娇俏帝少我嘴挑\\t总裁霸爱小小新娘要逃婚\\t花生怎么煮好吃\\t中国证券网\\t柔和双沟销售\\t中秋节的诗句图片\\t男子怪病喝洗洁精\\t4π等于多少\\t服装批发5元\\t怀孕33周肚子隐隐作痛怎么回事\\t百度云\\t酱炒蒜苔的家常做法\\t水煮花生米\\t天才小熊猫作品\\t袁姗姗\\t临泉长官镇桥口\\t呼作白玉盘的上一句\\t微信表情包搞笑图片\\t滴滴快车司机端\\t教师节手抄报简单好看\\t大冶公安局 派出所\\t柔和双沟业务待遇怎么样\\t为什么哺乳期不会有月经\\t临泉长官水上乐园\\t忐忑不安的意思\\t临泉长官李小刚家庭农场\\t电信电子发票怎么报销\\t岳不群\\t:http://pan.baidu.com/s/1plefcb5\\t临泉长官镇桥口李天龙农场\\t凉拌水煮花生米的做法\\t威灵仙的功效与作用\\thttp://pan.baidu.com/s/1o7hnpmy\\t鸽子汤的做法\\t战神伪高冷 天降医妃拐回家\\t白颠疯初期症状\\t天才小熊猫\\t首席萌妻咬一口\\t弩弓枪商城\\t三国演义小说\\t临泉长官镇桥口植物养殖基地\\t邮储银行手机银行客户端下载\\t煮花米怎么做好吃\\t英语在线翻译\\t糖醋鲤鱼\\tｗｗｗ．２０１６ｙｇ．ｃｏｍ\\t搞笑微信表情图片带字\\t新婚甜似火:鲜妻', 3: '二胎生一对\\t三国演义\\t关于教师节的手抄报\\thttp://m37189.mustfollow.vx.mvote.net/wx\\t文王国窖42度价格表\\t鱼汤的做法\\thttp://www.cswanda.com/weixin/game1/2016\\t临泉长官镇桥口私人农场\\t临泉长官镇桥口镇杨营\\t临泉长官李天龙家庭农场\\t李子树根部有脓包怎么回事\\t单手高速转牌\\t医学院在什么路上\\t徽信早上好动态表情\\t宝宝小名大全2016洋气\\t寂寞男女聊天记录截图\\thttps://yunpan.cn/oc6nhvmrg5j2ur\\t神将世界\\t美丽的秋天作文300字\\thttp://pan.baidu.com/s/1nu9uizn\\t钢弩\\t冰川世纪电影\\t全文\\t触手蓝烟\\t鱼的做法\\t金罐加多宝20罐\\t澳洲托运行李规定\\t15346171303@189.cn\\t炒蒜苔的家常做法\\t被时光掩埋的秘密\\t根号13.6等于几\\t方特东方神画\\t粉红花朵图片\\tqq号申请\\t千亿盛宠 大叔吻慢点\\thttp://linjiada1989.1688.com\\t东方财富网首页\\thttp://pan.baidu.com/s/1hraemhe\\t动力煤价格\\t手机遥控魔棒\\tjzg\\thttp://pan.baidu.com/s/1o8cxpmm\\t行李托运到澳洲\\t蚬壳胃散副作用\\t红烧鲤鱼\\t触手tv\\t中国财经信息网中财网\\t立方根800\\t美食菜谱\\t笑傲江湖电视剧\\t柔和双沟怎么样\\t笑傲江湖\\t花的品种名字及图片\\t滴滴司机端\\t奇怪君\\t鸡蛋灌饼\\t天龙农家乐园\\t吉拉拉歌词\\t陈翔的女朋友\\t牢解场的生活\\t微微一笑很倾城\\t豪门少奶奶谢少的心尖宠妻'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ_reduced.csv\n",
      "Header: [0]\n",
      "First row: {0: '00:00:00\\t2982199073774412\\t[360安全卫士]\\t8 3\\tdownload.it.com.cn/softweb/software/firewall/antivirus/20067/17938.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/.DS_Store\n",
      "Could not read .DS_Store: 'gb18030' codec can't decode byte 0xff in position 304: illegal multibyte sequence\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060816.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '1644912294011417\\t[伶俐饰品]\\t3 1\\twww.chuangye123.com/main/view2546.htm'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060801.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '011149014591546047\\t[cass+5.1+下载]\\t5 1\\t220.181.31.82/forum/content/212_225324_1.htm'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060805.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '20781048242866507\\t[earth]\\t5 1\\tdownload.it.com.cn/softweb/software/network/nethelper/20056/12277.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060812.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '28352766340529323\\t[黄鑫]\\t52 1\\tbbs.oyosky.net/dispbbs.asp?boardID=29&ID=30035&page=1'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060802.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '5630971727005645\\t[张玉凤]\\t6 1\\tnews.163.com/05/0304/13/1E0K4GBE00011246.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060815.decode.filter\n",
      "Could not read access_log.20060815.decode.filter: 'gb18030' codec can't decode byte 0xc0 in position 269757: illegal multibyte sequence\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060811.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '5365669143908531\\t[四柱预测]\\t101 1\\tzyilin.com/web/down_view.asp?id=25'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060806.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '4387942666435802\\t[超生事实认定]\\t4 1\\twww.440303.com/Book/default1.asp?PageNo=11&text=&option=0&jinghua='}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060828.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '5055489863142736\\t[鲁迅美术学院：2007年硕士研究生招生简章]\\t1 1\\twww.okhere.net/kaoyan/bkzn/zsjz/2007nzsjz/2007ln/110822792.shtml'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060822.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '6383203565086312\\t[bt种子下载]\\t8 1\\twww.lovetu.com/'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060808.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '3244372384901233\\t[什么是负离子]\\t1 1\\tnous.ea3w.com/2005/0720/10129.shtml'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060826.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '9008533481548073\\t[acd+see+软件下载]\\t9 1\\tcn.shareware-download.org/acd-see-6.0.php'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060831.decode.filter\n",
      "Could not read access_log.20060831.decode.filter: 'gb18030' codec can't decode byte 0xcc in position 266336: illegal multibyte sequence\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060818.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '41248200853120953\\t[成都中航地产+项目]\\t4 1\\tw.51sobu.com/new/content/2005128201135039101084.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060821.decode.filter\n",
      "Could not read access_log.20060821.decode.filter: 'gb18030' codec can't decode byte 0xc1 in position 213563: illegal multibyte sequence\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060824.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '6040453669197912\\t[yahoo+japan+ヤフー]\\t1 1\\twww.kantsuu.com/news1/20051014014600.shtml'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060820.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '9150182417968171\\t[郊外野合]\\t7 1\\tgentleman.zgjrw.com/News/200583/Lady/169351730910.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060819.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '2989561649501192\\t[违章+城管]\\t4 1\\tnews.jschina.com.cn/gb/jschina/news/jiangsu/njnews/userobject1ai471201.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060830.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '8708220754826088\\t[玉溪]\\t26 1\\tclever-yan.blog.sohu.com/3397233.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060827.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '004379262894437075\\t[排屋效果图]\\t2 1\\twww.dcsw.cn/cp_view.asp?id=962'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060809.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '40570680782592616\\t[如何下载三星铃音]\\t3 1\\t12530net.cn/shouji/lingyin-000780/index.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060823.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '11683194290172971\\t[babyvox图]\\t15 1\\twww.1000tu.com/article/35/2006/14245.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060829.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '0850241335161595\\t[陋俗]\\t3 1\\t36400.cn/sohu/luoti.htm'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060807.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '7482667275735597\\t[多乐士广告歌曲]\\t2 1\\tbbs.uptu.com/simple/index.php?t20207.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060810.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '8745103313366893\\t[305666612]\\t4 1\\t305666612.blog.sohu.com/6864516.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060814.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '4362770675201218\\t[flash+mtv]\\t1 1\\twww.9flash.com/'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060803.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '7828305877721831\\t[明星]\\t1 1\\tclub.chinaren.com/73366370.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060813.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '48645817418635084\\t[口塞口枷系列]\\t5 1\\twww.yao808.com/html/200605310127466768.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060804.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '56367486717939\\t[\"www.hbrs.gov.cn\"]\\t7 1\\twww.hbrsks.gov.cn/hbrs/messages/20050825/2660.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060817.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '7943712239025518\\t[麻花公主]\\t25 1\\twww.90ms.com/BBS/dispbbs.asp?boardID=5&ID=3412&page=1'}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 数据预处理",
   "id": "395f29704b1cbef2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T11:40:58.604138Z",
     "start_time": "2024-10-27T11:19:08.127534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "with open('data/sougou_competition_data/user_tag_query.10W.TRAIN', 'r', encoding='gb18030') as file:\n",
    "    # 读取文件内容并按换行符分隔\n",
    "    lines = file.read().splitlines()\n",
    "\n",
    "raw_queries = []\n",
    "for line in lines:\n",
    "    query = line.split('\\t')[4:]\n",
    "    raw_queries.extend(query)\n",
    " \n",
    "    \n",
    "import re\n",
    "unqualified_pattern = re.compile(\n",
    "    r'(http[s]?://\\S+|www\\.\\S+)'          # 匹配 URL\n",
    "    r'|(\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b)'  # 匹配邮箱\n",
    "    r'|(^[0-9.]+$)'                        # 仅含数字或特殊字符\n",
    ")\n",
    "def is_valid_query(query):\n",
    "    return not unqualified_pattern.search(query)\n",
    "\n",
    "queries = list(filter(is_valid_query, raw_queries))\n",
    "\n",
    "\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "query_words = []\n",
    "allowPOS = ['n']  # 允许的词性列表\n",
    "\n",
    "for query in queries:\n",
    "    a_query_words = []\n",
    "    # 使用带词性的分词\n",
    "    words = pseg.cut(query)\n",
    "    for word, flag in words:\n",
    "        # 仅保留指定词性且长度大于等于 2 的词\n",
    "        if (flag[0] in allowPOS) and len(word) >= 2:\n",
    "            a_query_words.append(word)\n",
    "    # 仅在分词后词数大于 1 时添加到 query_words\n",
    "    if len(a_query_words) > 1:\n",
    "        query_words.append(a_query_words)\n",
    "\n",
    "    \n",
    "# 保存数据到 CSV 文件\n",
    "with open('processed_data.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(query_words)\n"
   ],
   "id": "16d89bf1ff5d4f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/jieba/posseg/__init__.py:16: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  re_skip_detail = re.compile(\"([\\.0-9]+|[a-zA-Z0-9]+)\")\n",
      "/opt/anaconda3/lib/python3.12/site-packages/jieba/posseg/__init__.py:17: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  re_han_internal = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._]+)\")\n",
      "/opt/anaconda3/lib/python3.12/site-packages/jieba/posseg/__init__.py:18: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  re_skip_internal = re.compile(\"(\\r\\n|\\s)\")\n",
      "/opt/anaconda3/lib/python3.12/site-packages/jieba/posseg/__init__.py:21: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  re_num = re.compile(\"[\\.0-9]+\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[101], line 35\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# 使用带词性的分词\u001B[39;00m\n\u001B[1;32m     34\u001B[0m words \u001B[38;5;241m=\u001B[39m pseg\u001B[38;5;241m.\u001B[39mcut(query)\n\u001B[0;32m---> 35\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m word, flag \u001B[38;5;129;01min\u001B[39;00m words:\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;66;03m# 仅保留指定词性且长度大于等于 2 的词\u001B[39;00m\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (flag[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m allowPOS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(word) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m     38\u001B[0m         a_query_words\u001B[38;5;241m.\u001B[39mappend(word)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/jieba/posseg/__init__.py:294\u001B[0m, in \u001B[0;36mcut\u001B[0;34m(sentence, HMM, use_paddle)\u001B[0m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01mglobal\u001B[39;00m dt\n\u001B[1;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m jieba\u001B[38;5;241m.\u001B[39mpool \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 294\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m dt\u001B[38;5;241m.\u001B[39mcut(sentence, HMM\u001B[38;5;241m=\u001B[39mHMM):\n\u001B[1;32m    295\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m w\n\u001B[1;32m    296\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/jieba/posseg/__init__.py:249\u001B[0m, in \u001B[0;36mPOSTokenizer.cut\u001B[0;34m(self, sentence, HMM)\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcut\u001B[39m(\u001B[38;5;28mself\u001B[39m, sentence, HMM\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m--> 249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__cut_internal(sentence, HMM\u001B[38;5;241m=\u001B[39mHMM):\n\u001B[1;32m    250\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m w\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/jieba/posseg/__init__.py:226\u001B[0m, in \u001B[0;36mPOSTokenizer.__cut_internal\u001B[0;34m(self, sentence, HMM)\u001B[0m\n\u001B[1;32m    224\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m blk \u001B[38;5;129;01min\u001B[39;00m blocks:\n\u001B[1;32m    225\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m re_han_internal\u001B[38;5;241m.\u001B[39mmatch(blk):\n\u001B[0;32m--> 226\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m cut_blk(blk):\n\u001B[1;32m    227\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m word\n\u001B[1;32m    228\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/jieba/posseg/__init__.py:209\u001B[0m, in \u001B[0;36mPOSTokenizer.__cut_DAG\u001B[0;34m(self, sentence)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mFREQ\u001B[38;5;241m.\u001B[39mget(buf):\n\u001B[1;32m    208\u001B[0m     recognized \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__cut_detail(buf)\n\u001B[0;32m--> 209\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m recognized:\n\u001B[1;32m    210\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m t\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/jieba/posseg/__init__.py:139\u001B[0m, in \u001B[0;36mPOSTokenizer.__cut_detail\u001B[0;34m(self, sentence)\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m blk \u001B[38;5;129;01min\u001B[39;00m blocks:\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m re_han_detail\u001B[38;5;241m.\u001B[39mmatch(blk):\n\u001B[0;32m--> 139\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__cut(blk):\n\u001B[1;32m    140\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m word\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/jieba/posseg/__init__.py:118\u001B[0m, in \u001B[0;36mPOSTokenizer.__cut\u001B[0;34m(self, sentence)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__cut\u001B[39m(\u001B[38;5;28mself\u001B[39m, sentence):\n\u001B[0;32m--> 118\u001B[0m     prob, pos_list \u001B[38;5;241m=\u001B[39m viterbi(\n\u001B[1;32m    119\u001B[0m         sentence, char_state_tab_P, start_P, trans_P, emit_P)\n\u001B[1;32m    120\u001B[0m     begin, nexti \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, char \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(sentence):\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/jieba/posseg/viterbi.py:37\u001B[0m, in \u001B[0;36mviterbi\u001B[0;34m(obs, states, start_p, trans_p, emit_p)\u001B[0m\n\u001B[1;32m     34\u001B[0m     obs_states \u001B[38;5;241m=\u001B[39m prev_states_expect_next \u001B[38;5;28;01mif\u001B[39;00m prev_states_expect_next \u001B[38;5;28;01melse\u001B[39;00m all_states\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m y \u001B[38;5;129;01min\u001B[39;00m obs_states:\n\u001B[0;32m---> 37\u001B[0m     prob, state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m((V[t \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m][y0] \u001B[38;5;241m+\u001B[39m trans_p[y0]\u001B[38;5;241m.\u001B[39mget(y, MIN_INF) \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m     38\u001B[0m                        emit_p[y]\u001B[38;5;241m.\u001B[39mget(obs[t], MIN_FLOAT), y0) \u001B[38;5;28;01mfor\u001B[39;00m y0 \u001B[38;5;129;01min\u001B[39;00m prev_states)\n\u001B[1;32m     39\u001B[0m     V[t][y] \u001B[38;5;241m=\u001B[39m prob\n\u001B[1;32m     40\u001B[0m     mem_path[t][y] \u001B[38;5;241m=\u001B[39m state\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/jieba/posseg/viterbi.py:37\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     34\u001B[0m     obs_states \u001B[38;5;241m=\u001B[39m prev_states_expect_next \u001B[38;5;28;01mif\u001B[39;00m prev_states_expect_next \u001B[38;5;28;01melse\u001B[39;00m all_states\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m y \u001B[38;5;129;01min\u001B[39;00m obs_states:\n\u001B[0;32m---> 37\u001B[0m     prob, state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m((V[t \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m][y0] \u001B[38;5;241m+\u001B[39m trans_p[y0]\u001B[38;5;241m.\u001B[39mget(y, MIN_INF) \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m     38\u001B[0m                        emit_p[y]\u001B[38;5;241m.\u001B[39mget(obs[t], MIN_FLOAT), y0) \u001B[38;5;28;01mfor\u001B[39;00m y0 \u001B[38;5;129;01min\u001B[39;00m prev_states)\n\u001B[1;32m     39\u001B[0m     V[t][y] \u001B[38;5;241m=\u001B[39m prob\n\u001B[1;32m     40\u001B[0m     mem_path[t][y] \u001B[38;5;241m=\u001B[39m state\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 拆分出一个小数据集",
   "id": "a4e9ffecddbb11d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T04:15:33.806290Z",
     "start_time": "2024-10-27T04:15:31.958311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "# 原始文件路径\n",
    "original_file_path = 'processed_data.csv'\n",
    "# 新文件路径\n",
    "new_file_path = 'small_processed_data.csv'\n",
    "\n",
    "# 计算要抽取的行数\n",
    "with open(original_file_path, 'r', encoding='utf-8') as file:\n",
    "    total_lines = sum(1 for line in file)\n",
    "    sample_size = max(1, total_lines // 200)  # 确保至少有一行\n",
    "\n",
    "# 读取并写入前1/100的数据\n",
    "with open(original_file_path, 'r', encoding='utf-8') as file, \\\n",
    "     open(new_file_path, 'w', newline='', encoding='utf-8') as new_file:\n",
    "    reader = csv.reader(file)\n",
    "    writer = csv.writer(new_file)\n",
    "    \n",
    "    for i, row in enumerate(reader):\n",
    "        if i >= sample_size:\n",
    "            break\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"前 1/200 的数据已保存到 {new_file_path}\")"
   ],
   "id": "e4d07beb077ec9cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前 1/200 的数据已保存到 small_processed_data.csv\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 竞争性关键词模型类",
   "id": "22a95e6245e4098c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T11:04:27.295920Z",
     "start_time": "2024-10-27T11:04:27.239479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "class CompKey:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        # 初始化字典和变量\n",
    "        self.name = name\n",
    "        self.inters = {}         # 中介关键词字典\n",
    "        self.comps = {}          # 竞争关键词字典\n",
    "        self.inters_all = {}     # 每个中介关键词出现的总次数\n",
    "        self.result = {}         # 存放最终竞争度排名结果\n",
    "        self.keyword = ''        # 当前处理的种子关键词\n",
    "        self.num_queries = 0     # 当前种子关键词的查询数量\n",
    "        self.num_inters = 0      # 中介关键词数量\n",
    "        self.threshold = 0       # 中介关键词过滤阈值\n",
    "        self.comp_cache = {}     # 缓存关键词竞争度排名结果\n",
    "\n",
    "    def initialize(self, keyword):\n",
    "        \"\"\"初始化关键词相关数据\"\"\"\n",
    "        self.inters.clear()\n",
    "        self.comps.clear()\n",
    "        self.num_queries = 0\n",
    "        self.num_inters = 0\n",
    "        self.keyword = keyword\n",
    "\n",
    "    def train(self, data, keywords: list, num_results: int = 20):\n",
    "        \"\"\"训练模型，对每个关键词进行竞争性分析\"\"\"\n",
    "        for step, keyword in enumerate(keywords):\n",
    "            print(f\"Training keyword {step + 1}/{len(keywords)}: '{keyword}'\")\n",
    "            # 初始化\n",
    "            self.initialize(keyword)\n",
    "            # 筛选包含种子关键词的查询\n",
    "            filtered_data = self.filter_query(data)\n",
    "            # 分析中介关键词和竞争关键词\n",
    "            self.analyze_inter(filtered_data)\n",
    "            self.analyze_comp(data)\n",
    "            # 计算竞争度并缓存结果\n",
    "            self.comp_cache[keyword] = self.calculate_comp()[:num_results]\n",
    "            print(f\"Completed training for '{keyword}'\")\n",
    "    \n",
    "    def save_to_disk(self, filename=\"comp_cache.json\"):\n",
    "        \"\"\"将训练结果保存到磁盘上的文件中\"\"\"\n",
    "        try:\n",
    "            # 打开文件并以 JSON 格式写入 comp_cache\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.comp_cache, f, ensure_ascii=False, indent=4)\n",
    "            print(f\"Training results saved successfully to '{filename}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to disk: {e}\")\n",
    "\n",
    "    def predict(self, keyword):\n",
    "        \"\"\"预测指定关键词的竞争度排名\"\"\"\n",
    "        return self.comp_cache.get(keyword, None)\n",
    "\n",
    "    def filter_query(self, data):\n",
    "        \"\"\"筛选包含种子关键词的查询记录\"\"\"\n",
    "        print(\"Filtering queries...\")\n",
    "        start_time = time.time()\n",
    "        # 筛选出包含关键词的查询\n",
    "        filtered_data = [query for query in data if self.keyword in query]\n",
    "        self.num_queries = len(filtered_data)\n",
    "        end_time = time.time()\n",
    "        print(f\"Got {self.num_queries} queries related to keyword '{self.keyword}'. Time taken: {end_time - start_time:.2f}s\")\n",
    "        return filtered_data\n",
    "\n",
    "    def analyze_inter(self, data):\n",
    "        \"\"\"分析中介关键词\"\"\"\n",
    "        print(\"Analyzing intermediate keywords (inters)...\")\n",
    "        start_time = time.time()\n",
    "        for query in data:\n",
    "            for word in query:\n",
    "                if word != self.keyword:\n",
    "                    self.dict_add(self.inters, word)\n",
    "        inters_total = len(self.inters)\n",
    "        # 过滤出现次数较少的中介关键词\n",
    "        keys_to_delete = [key for key, count in self.inters.items() if count < self.threshold]  #TODO 这里的阈值需要调整\n",
    "        for key in keys_to_delete:\n",
    "            del self.inters[key]\n",
    "        self.num_inters = len(self.inters)\n",
    "        end_time = time.time()\n",
    "        print(f\"Got {self.num_inters}/{inters_total} inters after filtering. Time taken: {end_time - start_time:.2f}s\")\n",
    "\n",
    "    def analyze_comp(self, data):\n",
    "        \"\"\"分析竞争关键词\"\"\"\n",
    "        print(\"Analyzing competing keywords (comps)...\")\n",
    "        start_time = time.time()\n",
    "        with tqdm(total=self.num_inters, desc=\"Progress\", unit=\"keyword\") as pbar:\n",
    "            for step, key in enumerate(self.inters):\n",
    "                query_start_time = time.time()\n",
    "                # 找到包含中介关键词的查询\n",
    "                filtered_data = [query for query in data if key in query]\n",
    "                self.inters_all[key] = len(filtered_data)\n",
    "                for query in filtered_data:\n",
    "                    for word in query:\n",
    "                        if word != key and word != self.keyword:\n",
    "                            self.dict_add(self.comps, (word, key))\n",
    "                query_end_time = time.time()\n",
    "                # 更新进度条\n",
    "                pbar.set_postfix(time_per_keyword=f\"{query_end_time - query_start_time:.2f}s\")\n",
    "                pbar.update(1)  # 每处理一个关键词，更新一次进度条\n",
    "        end_time = time.time()\n",
    "        print(f\"Completed analyzing comps. Total comp-inter pairs: {len(self.comps)}. Time taken: {end_time - start_time:.2f}s\")\n",
    "\n",
    "    def calculate_comp(self):\n",
    "        \"\"\"计算竞争关键词的竞争度\"\"\"\n",
    "        print(\"Calculating competition scores...\")\n",
    "        start_time = time.time()\n",
    "        for comp, inter in self.comps:\n",
    "            num = self.comps[(comp, inter)]\n",
    "            value = self.result.get(comp, 0)\n",
    "            # 跳过没有竞争关键词的中介关键词\n",
    "            if self.inters_all[inter] == self.inters[inter]:\n",
    "                continue  # 跳过该中介关键词\n",
    "            \n",
    "            # 计算竞争度分数\n",
    "            self.result[comp] = value + (self.inters[inter] / self.num_queries) * (num / (self.inters_all[inter] - self.inters[inter]))\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Completed calculation of competition scores. Time taken: {end_time - start_time:.2f}s\")\n",
    "        return sorted(self.result.items(), key=lambda d: d[1], reverse=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def dict_add(_dict, key):\n",
    "        \"\"\"增加字典中 key 的计数\"\"\"\n",
    "        if key in _dict:\n",
    "            _dict[key] += 1\n",
    "        else:\n",
    "            _dict[key] = 1"
   ],
   "id": "f813a803acdeef98",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T11:41:22.010212Z",
     "start_time": "2024-10-27T11:41:21.976637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "class CompKey:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        # 初始化字典和变量\n",
    "        self.name = name\n",
    "        self.inters = {}         # 中介关键词字典\n",
    "        self.comps = {}          # 竞争关键词字典\n",
    "        self.inters_all = {}     # 每个中介关键词出现的总次数\n",
    "        self.result = {}         # 存放最终竞争度排名结果\n",
    "        self.keyword = ''        # 当前处理的种子关键词\n",
    "        self.num_queries = 0     # 当前种子关键词的查询数量\n",
    "        self.num_inters = 0      # 中介关键词数量\n",
    "        self.threshold = 0       # 中介关键词过滤阈值\n",
    "        self.comp_cache = {}     # 缓存关键词竞争度排名结果\n",
    "\n",
    "    def initialize(self, keyword):\n",
    "        \"\"\"初始化关键词相关数据\"\"\"\n",
    "        self.inters.clear()\n",
    "        self.comps.clear()\n",
    "        self.inters_all.clear()\n",
    "        self.result.clear()\n",
    "        self.num_queries = 0\n",
    "        self.num_inters = 0\n",
    "        self.keyword = keyword\n",
    "\n",
    "    def train(self, data, keywords: list, num_results: int = 20):\n",
    "        \"\"\"训练模型，对每个关键词进行竞争性分析\"\"\"\n",
    "        print(\"构建关键词到查询的映射（word_to_queries）...\")\n",
    "        word_to_queries = defaultdict(set)\n",
    "        for i, query in enumerate(data):\n",
    "            for word in set(query):  # 使用 set 来避免同一查询中重复的词\n",
    "                word_to_queries[word].add(i)\n",
    "        print(\"word_to_queries 构建完成。\")\n",
    "\n",
    "        for step, keyword in enumerate(keywords):\n",
    "            print(f\"Training keyword {step + 1}/{len(keywords)}: '{keyword}'\")\n",
    "            # 初始化\n",
    "            self.initialize(keyword)\n",
    "            # 筛选包含种子关键词的查询\n",
    "            filtered_query_indices = word_to_queries.get(keyword, set())\n",
    "            filtered_data = [data[i] for i in filtered_query_indices]\n",
    "            self.num_queries = len(filtered_data)\n",
    "            # 分析中介关键词和竞争关键词\n",
    "            self.analyze_inter(filtered_data)\n",
    "            self.analyze_comp(data, word_to_queries)\n",
    "            # 计算竞争度并缓存结果\n",
    "            self.comp_cache[keyword] = self.calculate_comp()[:num_results]\n",
    "            print(f\"Completed training for '{keyword}'\")\n",
    "\n",
    "    def save_to_disk(self, filename=\"comp_cache.json\"):\n",
    "        \"\"\"将训练结果保存到磁盘上的文件中\"\"\"\n",
    "        try:\n",
    "            # 打开文件并以 JSON 格式写入 comp_cache\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.comp_cache, f, ensure_ascii=False, indent=4)\n",
    "            print(f\"Training results saved successfully to '{filename}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to disk: {e}\")\n",
    "\n",
    "    def predict(self, keyword):\n",
    "        \"\"\"预测指定关键词的竞争度排名\"\"\"\n",
    "        return self.comp_cache.get(keyword, None)\n",
    "\n",
    "    def analyze_inter(self, filtered_data):\n",
    "        \"\"\"分析中介关键词\"\"\"\n",
    "        print(\"Analyzing intermediate keywords (inters)...\")\n",
    "        start_time = time.time()\n",
    "        for query in filtered_data:\n",
    "            for word in query:\n",
    "                if word != self.keyword:\n",
    "                    self.dict_add(self.inters, word)\n",
    "        inters_total = len(self.inters)\n",
    "        # 过滤出现次数较少的中介关键词\n",
    "        keys_to_delete = [key for key, count in self.inters.items() if count < self.threshold]\n",
    "        for key in keys_to_delete:\n",
    "            del self.inters[key]\n",
    "        self.num_inters = len(self.inters)\n",
    "        end_time = time.time()\n",
    "        print(f\"Got {self.num_inters}/{inters_total} inters after filtering. Time taken: {end_time - start_time:.2f}s\")\n",
    "\n",
    "    def analyze_comp(self, data, word_to_queries):\n",
    "        \"\"\"分析竞争关键词\"\"\"\n",
    "        print(\"Analyzing competing keywords (comps)...\")\n",
    "        start_time = time.time()\n",
    "        with tqdm(total=self.num_inters, desc=\"Progress\", unit=\"keyword\") as pbar:\n",
    "            for step, key in enumerate(self.inters):\n",
    "                query_start_time = time.time()\n",
    "                # 获取包含中介关键词的查询索引\n",
    "                inter_query_indices = word_to_queries.get(key, set())\n",
    "                self.inters_all[key] = len(inter_query_indices)\n",
    "                for i in inter_query_indices:\n",
    "                    query = data[i]\n",
    "                    for word in query:\n",
    "                        if word != key and word != self.keyword:\n",
    "                            self.dict_add(self.comps, (word, key))\n",
    "                query_end_time = time.time()\n",
    "                # 更新进度条\n",
    "                pbar.set_postfix(time_per_keyword=f\"{query_end_time - query_start_time:.2f}s\")\n",
    "                pbar.update(1)  # 每处理一个关键词，更新一次进度条\n",
    "        end_time = time.time()\n",
    "        print(f\"Completed analyzing comps. Total comp-inter pairs: {len(self.comps)}. Time taken: {end_time - start_time:.2f}s\")\n",
    "\n",
    "    def calculate_comp(self):\n",
    "        \"\"\"计算竞争关键词的竞争度\"\"\"\n",
    "        print(\"Calculating competition scores...\")\n",
    "        start_time = time.time()\n",
    "        for comp, inter in self.comps:\n",
    "            num = self.comps[(comp, inter)]\n",
    "            value = self.result.get(comp, 0)\n",
    "            # 跳过没有竞争关键词的中介关键词\n",
    "            if self.inters_all[inter] == self.inters[inter]:\n",
    "                continue  # 跳过该中介关键词\n",
    "\n",
    "            # 计算竞争度分数\n",
    "            self.result[comp] = value + (self.inters[inter] / self.num_queries) * (num / (self.inters_all[inter] - self.inters[inter]))\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Completed calculation of competition scores. Time taken: {end_time - start_time:.2f}s\")\n",
    "        return sorted(self.result.items(), key=lambda d: d[1], reverse=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def dict_add(_dict, key):\n",
    "        \"\"\"增加字典中 key 的计数\"\"\"\n",
    "        if key in _dict:\n",
    "            _dict[key] += 1\n",
    "        else:\n",
    "            _dict[key] = 1"
   ],
   "id": "ef006f495ce3a48e",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 测试读取\n",
    "完整的数据集读取可能要4分钟"
   ],
   "id": "adbcb45803f69f1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T11:44:37.276563Z",
     "start_time": "2024-10-27T11:42:06.570132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 从 CSV 文件读取数据\n",
    "with open('processed_data.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    data = [row for row in reader]\n",
    "\n"
   ],
   "id": "6a4c7ea2dcf7e52f",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 运行模型",
   "id": "5b7a1931cab75ff"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-27T11:45:17.999331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义要分析的关键词列表\n",
    "keywords = [\"华为\"]\n",
    "\n",
    "# 创建 CompKey 实例，指定模型名称\n",
    "comp_key_model = CompKey(name=\"KeywordCompetitionModel\")\n",
    "\n",
    "# 调用 train 方法进行训练，每个关键词返回前 10 个竞争关键词\n",
    "comp_key_model.train(data=data, keywords=keywords, num_results=10)\n",
    "comp_key_model.save_to_disk(\"comp_cache.json\")\n",
    "\n",
    "# 对某个关键词预测竞争度排名结果\n",
    "keyword_to_predict = \"华为\"\n",
    "competition_results = comp_key_model.predict(keyword_to_predict)\n",
    "\n",
    "# 输出预测结果\n",
    "if competition_results:\n",
    "    print(f\"竞争关键词排名结果 for '{keyword_to_predict}':\")\n",
    "    for rank, (comp_word, score) in enumerate(competition_results, 1):\n",
    "        print(f\"{rank}. {comp_word}: {score}\")\n",
    "else:\n",
    "    print(f\"No competition data found for keyword: {keyword_to_predict}\")"
   ],
   "id": "4caf81e150902a42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建关键词到查询的映射（word_to_queries）...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T04:22:49.430408Z",
     "start_time": "2024-10-27T04:22:49.359320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "# 统计词频\n",
    "word_counts = Counter(word for query in data for word in query)\n",
    "\n",
    "# 获取频率中等的10个词\n",
    "# 首先排序，然后取中间10个\n",
    "sorted_words = sorted(word_counts.items(), key=lambda item: item[1])\n",
    "middle_index = len(sorted_words) // 2\n",
    "middle_10_words = sorted_words[middle_index - 5: middle_index + 5]  # 取中间10个\n",
    "top_20_words = sorted_words[-200:-100]  # 取频率最高的10个\n",
    "bottom_10_words = sorted_words[:10]  # 取频率最低的10个\n",
    "\n",
    "# 输出结果\n",
    "print(\"Top 10 Words (Highest Frequency):\")\n",
    "for word, freq in top_20_words:\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n",
    "# print(\"\\nBottom 10 Words (Lowest Frequency):\")\n",
    "# for word, freq in bottom_10_words:\n",
    "#     print(f\"{word}: {freq}\")\n",
    "# \n",
    "# print(\"\\nMiddle 10 Words (Medium Frequency):\")\n",
    "# for word, freq in middle_10_words:\n",
    "#     print(f\"{word}: {freq}\")\n",
    "# "
   ],
   "id": "f68a07115c8a6344",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Words (Highest Frequency):\n",
      "句子: 127\n",
      "老: 127\n",
      "我们: 129\n",
      "个: 129\n",
      "乔任: 129\n",
      "密码: 129\n",
      "排行榜: 130\n",
      "微博: 130\n",
      "关于: 131\n",
      "dnf: 132\n",
      "网王: 133\n",
      "淘宝: 133\n",
      "天: 134\n",
      "叫: 135\n",
      "穿: 135\n",
      "为: 137\n",
      "by: 137\n",
      "4: 139\n",
      "西安: 139\n",
      "): 140\n",
      "开: 140\n",
      "手游: 140\n",
      "8: 141\n",
      "(: 144\n",
      "总裁: 146\n",
      "自己: 146\n",
      "名字: 148\n",
      "男生: 149\n",
      "7: 149\n",
      "皮肤: 149\n",
      "症状: 150\n",
      "就: 151\n",
      "答案: 154\n",
      "还: 155\n",
      "玩: 156\n",
      "没有: 157\n",
      "快: 158\n",
      "卡: 158\n",
      "华为: 160\n",
      "最新: 161\n",
      "路: 161\n",
      "不能: 161\n",
      "电话: 162\n",
      "联盟: 162\n",
      "打: 163\n",
      "需要: 163\n",
      "有限公司: 167\n",
      "少女: 168\n",
      "快递: 168\n",
      "还是: 169\n",
      "5: 170\n",
      "原因: 170\n",
      "歌曲: 172\n",
      "歌: 175\n",
      "/: 175\n",
      "来: 176\n",
      "一: 176\n",
      "攻略: 177\n",
      "全文: 178\n",
      "照片: 178\n",
      "区别: 179\n",
      "功效: 180\n",
      "图: 181\n",
      "作用: 181\n",
      "多: 182\n",
      "招聘: 182\n",
      "女: 185\n",
      "北京: 185\n",
      "写: 187\n",
      "买: 189\n",
      "方法: 191\n",
      "直播: 192\n",
      "医院: 192\n",
      "动漫: 194\n",
      "看: 195\n",
      "?: 195\n",
      "全集: 197\n",
      "最: 199\n",
      "歌词: 202\n",
      "lol: 204\n",
      "英文: 205\n",
      "汽车: 205\n",
      "1: 206\n",
      "免费: 207\n",
      "怎么回事: 207\n",
      "宝宝: 209\n",
      "作文: 211\n",
      "云: 212\n",
      "多久: 212\n",
      "新: 212\n",
      "对: 214\n",
      "怎样: 215\n",
      "微信: 215\n",
      "哪: 216\n",
      "系统: 218\n",
      "-: 222\n",
      "怀孕: 223\n",
      "在线: 224\n",
      "女生: 224\n",
      "读音: 224\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a02b0d27b15145e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
