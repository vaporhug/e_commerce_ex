{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-27T00:18:43.732035Z",
     "start_time": "2024-10-27T00:18:10.842811Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# 定义文件夹路径\n",
    "folder_path = 'data'\n",
    "\n",
    "def check_file_content_by_dir(file_path):\n",
    "    for root,dirs,files in os.walk(file_path):\n",
    "        for file in files:\n",
    "            if  file.endswith(('.txt','.zip')):\n",
    "                continue\n",
    "            print(f\"File: {os.path.join(root,file)}\")\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(root,file),encoding='gb18030',on_bad_lines='skip',header=None)\n",
    "                \n",
    "                \n",
    "                # 打印第一条数据\n",
    "                if not df.empty:\n",
    "                    print(\"First row:\", df.iloc[0].to_dict())\n",
    "                else:\n",
    "                    print(\"This file is empty.\")\n",
    "                \n",
    "                print(\"-\" * 40)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not read {file}: {e}\")\n",
    "\n",
    "check_file_content_by_dir('data')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: data/.DS_Store\n",
      "Could not read .DS_Store: 'gb18030' codec can't decode byte 0x80 in position 3131: illegal multibyte sequence\n",
      "File: data/sougou_competition_data/user_tag_query.10W.TEST\n",
      "Could not read user_tag_query.10W.TEST: 'gb18030' codec can't decode byte 0x80 in position 65767: illegal multibyte sequence\n",
      "File: data/sougou_competition_data/user_tag_query.10W.TRAIN\n",
      "Header: [0, 1, 2, 3]\n",
      "First row: {0: '22DD920316420BE2DF8D6EE651BA174B\\t1\\t1\\t4\\t柔和双沟\\t女生\\t中财网首页 财经\\thttp://pan.baidu.com/s/1plpjtn9\\t周公解梦大全查询2345\\t曹云金再讽郭德纲\\t总裁大人行行好\\t中财网第一财经传媒\\t教师节全文\\t男子砸毁15墓碑\\t黄岩岛最新填海图\\t引起的疲\\t缘来未迟落跑甜心不好惹\\t梁朝伟与替身同框\\t笑傲江湖电视剧任贤齐\\t小起名字女孩名字\\t海运行李到堪培拉\\t确定\\t诱爱99天 司少的天价宝贝\\t什么是遥控魔棒\\t徽信表情动态搞笑图片\\t教师节征文100字\\t安微联通网上营业厅\\t甜宠百分百:校草的萌萌未婚妻\\t豪门重生之暖爱成婚\\tnikehypershift和kd5哪个好看\\t韭菜炒鸡蛋\\t陈赫玩王者荣耀\\t虎牙楚河\\t三国演义小说txt下载\\t威县欧派\\t炒馍花怎么做好吃\\t黄岩岛最新消息2016年\\t中秋节诗句大全祝福\\t教师节征文\\t菜谱\\t柔和双沟卖的怎么样\\t七位数开奖结果\\t以色列停车场坍塌\\t天龙家庭农场\\t7.22什么星座\\t新旧约圣经和合本下载\\t4π\\twifi万能钥匙\\t威灵仙图片\\t临泉长官天龙家庭农场\\t早安总统大人\\t百合\\t莲藕的做法\\t花街\\t无锡\\t蚬壳胃散怎么吃\\t触手忆寒\\t中秋节的诗句\\t孟州电信 电子发票\\t鸡丝汤的做法\\t我等你\\t临泉长官镇桥口李小刚农场\\t朋仇\\t全民k歌\\t炸葱花\\t蒜苔炒肉\\t冰川的图片\\tkd5\\t…\\t若风\\t好奇纸尿裤\\t清蒸鱼\\t189.8是谁的平方\\t重庆餐馆发生爆炸\\t捡手机被失主抢劫\\thttps://yunpan.cn/ocsqfgtfya2ewj\\t炒馍花的家常做法\\t三国演义小说百度云\\t总裁掠爱小舅别太坏\\t:https://yunpan.cn/cmh8tmeyraiww\\t周公解梦\\t查坦克冰川\\t凉拌藕片的做法\\t投票\\t鸡丝炒什么好吃\\t被时光掩埋的秘密小说下载\\t中国电信电子发票\\t张续豪\\t关于月亮的诗句\\t用酵母蒸馒头的方法\\t赵丽颖碧瑶坐\\t触手兵长\\t图集 下载腾讯新闻', 1: '看街头混战武警\\t厦门航空\\t蚬壳胃散\\t炒茄子做法\\t身份类别怎么填\\t最好的我们里面的方特在哪里\\t牢里面的生活是怎样的\\t强迫症有哪些表现\\t白袍法师暖暖图片\\t朋仇广场舞\\t小宇热游\\t蒸馒头的方法\\t狡滑的意思\\t黄石大冶东岳派出所服务电话\\t三国演义小说下载txt\\thttp://zxjhjc9088.1688.com\\t松柏道馆\\t10.1高速免费几天\\t三国演义小说txt\\t柔和双沟业务待遇\\t酵母蒸馒头的方法\\t初中家教一对一辅导\\t口子窖\\t中秋节祝福诗句\\t侠岚\\t文王国窖42度价格表1001文王国窖42度价格表\\t批注是什么意思\\t殿下专属小丫头\\t无锡爆炸\\t炸茄子做法\\thttp://pan.baidu.com/s/1cor7gy\\t大件行李邮寄\\t烟火陈翔\\t没想到', 2: '真没想到作文\\t安徽滁州石坝镇\\t虎牙小宇\\t驾校培训跑长途\\t骨质性关节炎\\t左膝盖内侧疼是怎么回事\\t虎皮尖椒的做法大全\\t陈翔的女朋友吻照\\tq我的世界、5\\t23.04的平方根\\t神将世界表情包\\t寻找成龙\\t柔和双沟卖的\\t三国演义txt百度云\\t一般现在时\\t澳洲邮寄行李费用\\t触手若月\\t1991年11月26日是什么星座\\t校草成长记\\t暮光女向女友求婚\\t钢弩的价格图片\\t乐乐课堂\\t宠妻成瘾老婆你要乖\\t魔手tv\\t梅河口到济南的火车票\\t临泉长官镇\\t君子兰\\t南洋十大邪术电影\\t肚\\t炸油条的做法和配方\\t根号6等于多少\\t笑笑昨日帝王骗\\t吃惊的什么填词语\\t50字教师节征文\\t朝阳区黑庄户邮编\\t千百鲁\\t1991年农历11月26日是什么星座\\t圣经和合本免费下载\\t水煮花生米的做法\\thttp://pan.baidu.com/s/1jhbv9pg\\t十字弓\\t徽信表情含义\\t天才小熊猫微博长图\\t宠冠六宫:帝王的娇蛮皇妃\\t去广告软件 安卓版\\t萌妻娇俏帝少我嘴挑\\t总裁霸爱小小新娘要逃婚\\t花生怎么煮好吃\\t中国证券网\\t柔和双沟销售\\t中秋节的诗句图片\\t男子怪病喝洗洁精\\t4π等于多少\\t服装批发5元\\t怀孕33周肚子隐隐作痛怎么回事\\t百度云\\t酱炒蒜苔的家常做法\\t水煮花生米\\t天才小熊猫作品\\t袁姗姗\\t临泉长官镇桥口\\t呼作白玉盘的上一句\\t微信表情包搞笑图片\\t滴滴快车司机端\\t教师节手抄报简单好看\\t大冶公安局 派出所\\t柔和双沟业务待遇怎么样\\t为什么哺乳期不会有月经\\t临泉长官水上乐园\\t忐忑不安的意思\\t临泉长官李小刚家庭农场\\t电信电子发票怎么报销\\t岳不群\\t:http://pan.baidu.com/s/1plefcb5\\t临泉长官镇桥口李天龙农场\\t凉拌水煮花生米的做法\\t威灵仙的功效与作用\\thttp://pan.baidu.com/s/1o7hnpmy\\t鸽子汤的做法\\t战神伪高冷 天降医妃拐回家\\t白颠疯初期症状\\t天才小熊猫\\t首席萌妻咬一口\\t弩弓枪商城\\t三国演义小说\\t临泉长官镇桥口植物养殖基地\\t邮储银行手机银行客户端下载\\t煮花米怎么做好吃\\t英语在线翻译\\t糖醋鲤鱼\\tｗｗｗ．２０１６ｙｇ．ｃｏｍ\\t搞笑微信表情图片带字\\t新婚甜似火:鲜妻', 3: '二胎生一对\\t三国演义\\t关于教师节的手抄报\\thttp://m37189.mustfollow.vx.mvote.net/wx\\t文王国窖42度价格表\\t鱼汤的做法\\thttp://www.cswanda.com/weixin/game1/2016\\t临泉长官镇桥口私人农场\\t临泉长官镇桥口镇杨营\\t临泉长官李天龙家庭农场\\t李子树根部有脓包怎么回事\\t单手高速转牌\\t医学院在什么路上\\t徽信早上好动态表情\\t宝宝小名大全2016洋气\\t寂寞男女聊天记录截图\\thttps://yunpan.cn/oc6nhvmrg5j2ur\\t神将世界\\t美丽的秋天作文300字\\thttp://pan.baidu.com/s/1nu9uizn\\t钢弩\\t冰川世纪电影\\t全文\\t触手蓝烟\\t鱼的做法\\t金罐加多宝20罐\\t澳洲托运行李规定\\t15346171303@189.cn\\t炒蒜苔的家常做法\\t被时光掩埋的秘密\\t根号13.6等于几\\t方特东方神画\\t粉红花朵图片\\tqq号申请\\t千亿盛宠 大叔吻慢点\\thttp://linjiada1989.1688.com\\t东方财富网首页\\thttp://pan.baidu.com/s/1hraemhe\\t动力煤价格\\t手机遥控魔棒\\tjzg\\thttp://pan.baidu.com/s/1o8cxpmm\\t行李托运到澳洲\\t蚬壳胃散副作用\\t红烧鲤鱼\\t触手tv\\t中国财经信息网中财网\\t立方根800\\t美食菜谱\\t笑傲江湖电视剧\\t柔和双沟怎么样\\t笑傲江湖\\t花的品种名字及图片\\t滴滴司机端\\t奇怪君\\t鸡蛋灌饼\\t天龙农家乐园\\t吉拉拉歌词\\t陈翔的女朋友\\t牢解场的生活\\t微微一笑很倾城\\t豪门少奶奶谢少的心尖宠妻'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ_reduced.csv\n",
      "Header: [0]\n",
      "First row: {0: '00:00:00\\t2982199073774412\\t[360安全卫士]\\t8 3\\tdownload.it.com.cn/softweb/software/firewall/antivirus/20067/17938.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/.DS_Store\n",
      "Could not read .DS_Store: 'gb18030' codec can't decode byte 0xff in position 304: illegal multibyte sequence\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060816.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '1644912294011417\\t[伶俐饰品]\\t3 1\\twww.chuangye123.com/main/view2546.htm'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060801.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '011149014591546047\\t[cass+5.1+下载]\\t5 1\\t220.181.31.82/forum/content/212_225324_1.htm'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060805.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '20781048242866507\\t[earth]\\t5 1\\tdownload.it.com.cn/softweb/software/network/nethelper/20056/12277.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060812.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '28352766340529323\\t[黄鑫]\\t52 1\\tbbs.oyosky.net/dispbbs.asp?boardID=29&ID=30035&page=1'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060802.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '5630971727005645\\t[张玉凤]\\t6 1\\tnews.163.com/05/0304/13/1E0K4GBE00011246.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060815.decode.filter\n",
      "Could not read access_log.20060815.decode.filter: 'gb18030' codec can't decode byte 0xc0 in position 269757: illegal multibyte sequence\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060811.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '5365669143908531\\t[四柱预测]\\t101 1\\tzyilin.com/web/down_view.asp?id=25'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060806.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '4387942666435802\\t[超生事实认定]\\t4 1\\twww.440303.com/Book/default1.asp?PageNo=11&text=&option=0&jinghua='}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060828.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '5055489863142736\\t[鲁迅美术学院：2007年硕士研究生招生简章]\\t1 1\\twww.okhere.net/kaoyan/bkzn/zsjz/2007nzsjz/2007ln/110822792.shtml'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060822.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '6383203565086312\\t[bt种子下载]\\t8 1\\twww.lovetu.com/'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060808.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '3244372384901233\\t[什么是负离子]\\t1 1\\tnous.ea3w.com/2005/0720/10129.shtml'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060826.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '9008533481548073\\t[acd+see+软件下载]\\t9 1\\tcn.shareware-download.org/acd-see-6.0.php'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060831.decode.filter\n",
      "Could not read access_log.20060831.decode.filter: 'gb18030' codec can't decode byte 0xcc in position 266336: illegal multibyte sequence\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060818.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '41248200853120953\\t[成都中航地产+项目]\\t4 1\\tw.51sobu.com/new/content/2005128201135039101084.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060821.decode.filter\n",
      "Could not read access_log.20060821.decode.filter: 'gb18030' codec can't decode byte 0xc1 in position 213563: illegal multibyte sequence\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060824.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '6040453669197912\\t[yahoo+japan+ヤフー]\\t1 1\\twww.kantsuu.com/news1/20051014014600.shtml'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060820.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '9150182417968171\\t[郊外野合]\\t7 1\\tgentleman.zgjrw.com/News/200583/Lady/169351730910.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060819.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '2989561649501192\\t[违章+城管]\\t4 1\\tnews.jschina.com.cn/gb/jschina/news/jiangsu/njnews/userobject1ai471201.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060830.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '8708220754826088\\t[玉溪]\\t26 1\\tclever-yan.blog.sohu.com/3397233.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060827.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '004379262894437075\\t[排屋效果图]\\t2 1\\twww.dcsw.cn/cp_view.asp?id=962'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060809.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '40570680782592616\\t[如何下载三星铃音]\\t3 1\\t12530net.cn/shouji/lingyin-000780/index.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060823.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '11683194290172971\\t[babyvox图]\\t15 1\\twww.1000tu.com/article/35/2006/14245.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060829.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '0850241335161595\\t[陋俗]\\t3 1\\t36400.cn/sohu/luoti.htm'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060807.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '7482667275735597\\t[多乐士广告歌曲]\\t2 1\\tbbs.uptu.com/simple/index.php?t20207.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060810.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '8745103313366893\\t[305666612]\\t4 1\\t305666612.blog.sohu.com/6864516.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060814.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '4362770675201218\\t[flash+mtv]\\t1 1\\twww.9flash.com/'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060803.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '7828305877721831\\t[明星]\\t1 1\\tclub.chinaren.com/73366370.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060813.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '48645817418635084\\t[口塞口枷系列]\\t5 1\\twww.yao808.com/html/200605310127466768.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060804.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '56367486717939\\t[\"www.hbrs.gov.cn\"]\\t7 1\\twww.hbrsks.gov.cn/hbrs/messages/20050825/2660.html'}\n",
      "----------------------------------------\n",
      "File: data/sougou_search_log/SogouQ/access_log.20060817.decode.filter\n",
      "Header: [0]\n",
      "First row: {0: '7943712239025518\\t[麻花公主]\\t25 1\\twww.90ms.com/BBS/dispbbs.asp?boardID=5&ID=3412&page=1'}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 数据预处理",
   "id": "395f29704b1cbef2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T01:08:36.428696Z",
     "start_time": "2024-10-30T01:08:33.417283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "with open('data/sougou_competition_data/user_tag_query.10W.TRAIN', 'r', encoding='gb18030') as file:\n",
    "    # 读取文件内容并按换行符分隔\n",
    "    lines = file.read().splitlines()\n",
    "\n",
    "raw_queries = []\n",
    "for line in lines:\n",
    "    query = line.split('\\t')[4:]\n",
    "    raw_queries.extend(query)"
   ],
   "id": "d17445db49641f2a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T11:01:13.287832Z",
     "start_time": "2024-11-11T11:01:09.863589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "# from autocorrect import Speller\n",
    "from pycorrector import Corrector\n",
    "corrector = Corrector()\n",
    "\n",
    "correct_queries = []\n",
    "count = 0\n",
    "for query in tqdm(raw_queries):\n",
    "    blob = corrector.correct(query)\n",
    "    correct_queries.append(blob)\n",
    "    if query != blob['target']:\n",
    "        count += 1\n",
    "        # print(f\"Original: {query}\")\n",
    "        # print(f\"Corrected: {blob}\")\n",
    "# print(count)\n",
    "\n",
    "\n"
   ],
   "id": "ae46bf28ced5a875",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# from autocorrect import Speller\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpycorrector\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Corrector\n\u001B[1;32m      4\u001B[0m corrector \u001B[38;5;241m=\u001B[39m Corrector()\n\u001B[1;32m      6\u001B[0m correct_queries \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/pycorrector/__init__.py:10\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpycorrector\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcorrector\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Corrector\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpycorrector\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfusion_corrector\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ConfusionCorrector\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpycorrector\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdeepcontext\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdeepcontext_corrector\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DeepContextCorrector\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpycorrector\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdetector\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Detector\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpycorrector\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdetector\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m USER_DATA_DIR\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/pycorrector/deepcontext/deepcontext_corrector.py:12\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtime\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m List\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mloguru\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m logger\n\u001B[1;32m     15\u001B[0m sys\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../..\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/__init__.py:367\u001B[0m\n\u001B[1;32m    365\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m USE_GLOBAL_DEPS:\n\u001B[1;32m    366\u001B[0m         _load_global_deps()\n\u001B[0;32m--> 367\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_C\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m  \u001B[38;5;66;03m# noqa: F403\u001B[39;00m\n\u001B[1;32m    370\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mSymInt\u001B[39;00m:\n\u001B[1;32m    371\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    372\u001B[0m \u001B[38;5;124;03m    Like an int (including magic methods), but redirects all operations on the\u001B[39;00m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;124;03m    wrapped node. This is used in particular to symbolically record operations\u001B[39;00m\n\u001B[1;32m    374\u001B[0m \u001B[38;5;124;03m    in the symbolic shape workflow.\u001B[39;00m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:463\u001B[0m, in \u001B[0;36m_lock_unlock_module\u001B[0;34m(name)\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T11:56:14.015912Z",
     "start_time": "2024-10-28T11:56:08.982105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    \n",
    "import re\n",
    "unqualified_pattern = re.compile(\n",
    "    r'(http[s]?://\\S+|www\\.\\S+)'          # 匹配 URL\n",
    "    r'|(\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b)'  # 匹配邮箱\n",
    "    r'|(^[0-9.]+$)'                        # 仅含数字或特殊字符\n",
    ")\n",
    "def is_valid_query(query):\n",
    "    return not unqualified_pattern.search(query)\n",
    "\n",
    "queries = list(filter(is_valid_query, raw_queries))\n",
    "\n",
    "\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "query_words = []\n",
    "allowPOS = ['n']  # 允许的词性列表\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 假设 allowPOS 和 query_words 已经定义\n",
    "# allowPOS = set(...)  \n",
    "# query_words = []\n",
    "\n",
    "# 使用 tqdm 为查询列表添加进度条\n",
    "for query in tqdm(queries, desc=\"Processing queries\"):\n",
    "    a_query_words = []\n",
    "    # 使用带词性的分词\n",
    "    words = pseg.cut(query)\n",
    "    for word, flag in words:\n",
    "        # 仅保留指定词性且长度大于等于 2 的词\n",
    "        if (flag[0] in allowPOS) and len(word) >= 2:\n",
    "            a_query_words.append(word)\n",
    "    # 仅在分词后词数大于 1 时添加到 query_words\n",
    "    if len(a_query_words) > 1:\n",
    "        query_words.append(a_query_words)\n",
    "\n",
    "    \n",
    "# 保存数据到 CSV 文件\n",
    "with open('processed_data.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(query_words)\n"
   ],
   "id": "16d89bf1ff5d4f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-80482:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80483:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80485:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80486:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80484:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80487:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80488:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80489:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80490:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80491:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80492:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80493:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80494:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80495:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80496:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80497:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80498:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80499:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80500:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80501:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80502:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80503:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80504:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80505:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80506:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80507:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80508:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80509:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80510:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80511:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80512:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80513:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80514:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 19, in <module>\n",
      "    from . import util\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/util.py\", line 13, in <module>\n",
      "    import weakref\n",
      "  File \"<frozen importlib._bootstrap>\", line 1357, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 418, in __enter__\n",
      "  File \"<frozen importlib._bootstrap>\", line 311, in acquire\n",
      "  File \"<frozen importlib._bootstrap>\", line 175, in __exit__\n",
      "KeyboardInterrupt\n",
      "Process SpawnPoolWorker-80516:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80517:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80518:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80519:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80520:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80521:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80522:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80523:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80524:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80525:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-80526:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_query' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 22\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_valid_query\u001B[39m(query):\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m unqualified_pattern\u001B[38;5;241m.\u001B[39msearch(query)\n\u001B[0;32m---> 22\u001B[0m queries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mfilter\u001B[39m(is_valid_query, raw_queries))\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjieba\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjieba\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mposseg\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpseg\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[8], line 20\u001B[0m, in \u001B[0;36mis_valid_query\u001B[0;34m(query)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_valid_query\u001B[39m(query):\n\u001B[0;32m---> 20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m unqualified_pattern\u001B[38;5;241m.\u001B[39msearch(query)\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_312_64.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_312_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:764\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_312_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/Applications/PyCharm Professional Edition.app/Contents/plugins/python/helpers-pro/jupyter_debug/pydev_jupyter_plugin.py:90\u001B[0m, in \u001B[0;36mcan_not_skip\u001B[0;34m(plugin, pydb, frame, info)\u001B[0m\n\u001B[1;32m     86\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m---> 90\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcan_not_skip\u001B[39m(plugin, pydb, frame, info):\n\u001B[1;32m     91\u001B[0m     step_cmd \u001B[38;5;241m=\u001B[39m info\u001B[38;5;241m.\u001B[39mpydev_step_cmd\n\u001B[1;32m     92\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m step_cmd \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m108\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m _is_equals(frame, _get_stop_frame(info)):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 拆分出一个小数据集",
   "id": "a4e9ffecddbb11d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T11:57:43.319394Z",
     "start_time": "2024-10-28T11:57:42.544533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "# 原始文件路径\n",
    "original_file_path = 'processed_data.csv'\n",
    "# 新文件路径\n",
    "new_file_path = 'small_processed_data.csv'\n",
    "\n",
    "# 计算要抽取的行数\n",
    "with open(original_file_path, 'r', encoding='utf-8') as file:\n",
    "    total_lines = sum(1 for line in file)\n",
    "    sample_size = max(1, total_lines // 200)  # 确保至少有一行\n",
    "\n",
    "# 读取并写入前1/100的数据\n",
    "with open(original_file_path, 'r', encoding='utf-8') as file, \\\n",
    "     open(new_file_path, 'w', newline='', encoding='utf-8') as new_file:\n",
    "    reader = csv.reader(file)\n",
    "    writer = csv.writer(new_file)\n",
    "    \n",
    "    for i, row in enumerate(reader):\n",
    "        if i >= sample_size:\n",
    "            break\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"前 1/200 的数据已保存到 {new_file_path}\")"
   ],
   "id": "e4d07beb077ec9cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前 1/200 的数据已保存到 small_processed_data.csv\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 竞争性关键词模型类",
   "id": "22a95e6245e4098c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T15:45:10.410508Z",
     "start_time": "2024-10-29T15:45:10.400943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CompKey:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        # 初始化字典和变量\n",
    "        self.name = name\n",
    "        self.inters = {}         # 中介关键词字典\n",
    "        self.comps = {}          # 竞争关键词字典\n",
    "        self.inters_all = {}     # 每个中介关键词出现的总次数\n",
    "        self.result = {}         # 存放最终竞争度排名结果\n",
    "        self.keyword = ''        # 当前处理的种子关键词\n",
    "        self.num_queries = 0     # 当前种子关键词的查询数量\n",
    "        self.num_inters = 0      # 中介关键词数量\n",
    "        self.threshold = 0       # 中介关键词过滤阈值\n",
    "        self.comp_cache = {}     # 缓存关键词竞争度排名结果\n",
    "\n",
    "    def initialize(self, keyword):\n",
    "        \"\"\"初始化关键词相关数据\"\"\"\n",
    "        self.inters.clear()\n",
    "        self.comps.clear()\n",
    "        self.num_queries = 0\n",
    "        self.num_inters = 0\n",
    "        self.keyword = keyword\n",
    "\n",
    "    def train(self, data, keywords: list, num_results: int = 20):\n",
    "        \"\"\"训练模型，对每个关键词进行竞争性分析\"\"\"\n",
    "        for step, keyword in enumerate(keywords):\n",
    "            print(f\"Training keyword {step + 1}/{len(keywords)}: '{keyword}'\")\n",
    "            # 初始化\n",
    "            self.initialize(keyword)\n",
    "            # 筛选包含种子关键词的查询\n",
    "            filtered_data = self.filter_query(data)\n",
    "            # 分析中介关键词和竞争关键词\n",
    "            self.analyze_inter(filtered_data)\n",
    "            self.analyze_comp(data)\n",
    "            # 计算竞争度并缓存结果\n",
    "            self.comp_cache[keyword] = self.calculate_comp()[:num_results]\n",
    "            print(f\"Completed training for '{keyword}'\")\n",
    "    \n",
    "    def save_to_disk(self, filename=\"comp_cache.json\"):\n",
    "        \"\"\"将训练结果保存到磁盘上的文件中\"\"\"\n",
    "        try:\n",
    "            # 打开文件并以 JSON 格式写入 comp_cache\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.comp_cache, f, ensure_ascii=False, indent=4)\n",
    "            print(f\"Training results saved successfully to '{filename}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to disk: {e}\")\n",
    "\n",
    "    def predict(self, keyword):\n",
    "        \"\"\"预测指定关键词的竞争度排名\"\"\"\n",
    "        return self.comp_cache.get(keyword, None)\n",
    "\n",
    "    def filter_query(self, data):\n",
    "        \"\"\"筛选包含种子关键词的查询记录\"\"\"\n",
    "        print(\"Filtering queries...\")\n",
    "        start_time = time.time()\n",
    "        # 筛选出包含关键词的查询\n",
    "        filtered_data = [query for query in data if self.keyword in query]\n",
    "        self.num_queries = len(filtered_data)\n",
    "        end_time = time.time()\n",
    "        print(f\"Got {self.num_queries} queries related to keyword '{self.keyword}'. Time taken: {end_time - start_time:.2f}s\")\n",
    "        return filtered_data\n",
    "\n",
    "    def analyze_inter(self, data):\n",
    "        \"\"\"分析中介关键词\"\"\"\n",
    "        print(\"Analyzing intermediate keywords (inters)...\")\n",
    "        start_time = time.time()\n",
    "        for query in data:\n",
    "            for word in query:\n",
    "                if word != self.keyword:\n",
    "                    self.dict_add(self.inters, word)\n",
    "        inters_total = len(self.inters)\n",
    "        # 过滤出现次数较少的中介关键词\n",
    "        keys_to_delete = [key for key, count in self.inters.items() if count < self.threshold]  #TODO 这里的阈值需要调整\n",
    "        for key in keys_to_delete:\n",
    "            del self.inters[key]\n",
    "        self.num_inters = len(self.inters)\n",
    "        end_time = time.time()\n",
    "        print(f\"Got {self.num_inters}/{inters_total} inters after filtering. Time taken: {end_time - start_time:.2f}s\")\n",
    "\n",
    "    def analyze_comp(self, data):\n",
    "        \"\"\"分析竞争关键词\"\"\"\n",
    "        print(\"Analyzing competing keywords (comps)...\")\n",
    "        start_time = time.time()\n",
    "        with tqdm(total=self.num_inters, desc=\"Progress\", unit=\"keyword\") as pbar:\n",
    "            for step, key in enumerate(self.inters):\n",
    "                query_start_time = time.time()\n",
    "                # 找到包含中介关键词的查询\n",
    "                filtered_data = [query for query in data if key in query]\n",
    "                self.inters_all[key] = len(filtered_data)\n",
    "                for query in filtered_data:\n",
    "                    for word in query:\n",
    "                        if word != key and word != self.keyword:\n",
    "                            self.dict_add(self.comps, (word, key))\n",
    "                query_end_time = time.time()\n",
    "                # 更新进度条\n",
    "                pbar.set_postfix(time_per_keyword=f\"{query_end_time - query_start_time:.2f}s\")\n",
    "                pbar.update(1)  # 每处理一个关键词，更新一次进度条\n",
    "        end_time = time.time()\n",
    "        print(f\"Completed analyzing comps. Total comp-inter pairs: {len(self.comps)}. Time taken: {end_time - start_time:.2f}s\")\n",
    "\n",
    "    def calculate_comp(self):\n",
    "        \"\"\"计算竞争关键词的竞争度\"\"\"\n",
    "        print(\"Calculating competition scores...\")\n",
    "        start_time = time.time()\n",
    "        for comp, inter in self.comps:\n",
    "            num = self.comps[(comp, inter)]\n",
    "            value = self.result.get(comp, 0)\n",
    "            # 跳过没有竞争关键词的中介关键词\n",
    "            if self.inters_all[inter] == self.inters[inter]:\n",
    "                continue  # 跳过该中介关键词\n",
    "            \n",
    "            # 计算竞争度分数\n",
    "            self.result[comp] = value + (self.inters[inter] / self.num_queries) * (num / (self.inters_all[inter] - self.inters[inter]))\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Completed calculation of competition scores. Time taken: {end_time - start_time:.2f}s\")\n",
    "        return sorted(self.result.items(), key=lambda d: d[1], reverse=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def dict_add(_dict, key):\n",
    "        \"\"\"增加字典中 key 的计数\"\"\"\n",
    "        if key in _dict:\n",
    "            _dict[key] += 1\n",
    "        else:\n",
    "            _dict[key] = 1"
   ],
   "id": "f813a803acdeef98",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T15:45:07.188368Z",
     "start_time": "2024-10-29T15:45:07.171769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "class CompKeyLL:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        # 初始化字典和变量\n",
    "        self.name = name\n",
    "        self.inters = {}         # 中介关键词字典\n",
    "        self.comps = {}          # 竞争关键词字典\n",
    "        self.inters_all = {}     # 每个中介关键词出现的总次数\n",
    "        self.result = {}         # 存放最终竞争度排名结果\n",
    "        self.keyword = ''        # 当前处理的种子关键词\n",
    "        self.num_queries = 0     # 当前种子关键词的查询数量\n",
    "        self.num_inters = 0      # 中介关键词数量\n",
    "        self.threshold = 0       # 中介关键词过滤阈值\n",
    "        self.comp_cache = {}     # 缓存关键词竞争度排名结果\n",
    "\n",
    "    def initialize(self, keyword):\n",
    "        \"\"\"初始化关键词相关数据\"\"\"\n",
    "        self.inters.clear()\n",
    "        self.comps.clear()\n",
    "        self.inters_all.clear()\n",
    "        self.result.clear()\n",
    "        self.num_queries = 0\n",
    "        self.num_inters = 0\n",
    "        self.keyword = keyword\n",
    "\n",
    "    def train(self, data, keywords: list, num_results: int = 20):\n",
    "        \"\"\"训练模型，对每个关键词进行竞争性分析\"\"\"\n",
    "        print(\"构建关键词到查询的映射（word_to_queries）...\")\n",
    "        word_to_queries = defaultdict(set)\n",
    "        \n",
    "        for i, query in enumerate(data):\n",
    "            for word in set(query):  # 使用 set 来避免同一查询中重复的词\n",
    "                word_to_queries[word].add(i)\n",
    "        print(\"word_to_queries 构建完成。\")\n",
    "        num_indices = len(word_to_queries)\n",
    "        print(f\"Got {num_indices} unique words in the dataset.\")\n",
    "        print(\"前5个关键词及其查询索引:\")\n",
    "        for word, query_indices in list(word_to_queries.items())[:5]:\n",
    "            print(f\"关键词 '{word}' 出现在查询索引: {sorted(query_indices)}\")\n",
    "\n",
    "        for step, keyword in enumerate(keywords):\n",
    "            print(f\"Training keyword {step + 1}/{len(keywords)}: '{keyword}'\")\n",
    "            # 初始化\n",
    "            self.initialize(keyword)\n",
    "            # 筛选包含种子关键词的查询\n",
    "            filtered_query_indices = word_to_queries.get(keyword, set())\n",
    "            filtered_data = [data[i] for i in filtered_query_indices]\n",
    "            self.num_queries = len(filtered_data)\n",
    "            # 分析中介关键词和竞争关键词\n",
    "            self.analyze_inter(filtered_data)\n",
    "            self.analyze_comp(data, word_to_queries)\n",
    "            # 计算竞争度并缓存结果\n",
    "            self.comp_cache[keyword] = self.calculate_comp()[:num_results]\n",
    "            print(f\"Completed training for '{keyword}'\")\n",
    "\n",
    "    def save_to_disk(self, filename=\"comp_cache.json\"):\n",
    "        \"\"\"将训练结果保存到磁盘上的文件中\"\"\"\n",
    "        try:\n",
    "            # 打开文件并以 JSON 格式写入 comp_cache\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.comp_cache, f, ensure_ascii=False, indent=4)\n",
    "            print(f\"Training results saved successfully to '{filename}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to disk: {e}\")\n",
    "\n",
    "    def predict(self, keyword):\n",
    "        \"\"\"预测指定关键词的竞争度排名\"\"\"\n",
    "        return self.comp_cache.get(keyword, None)\n",
    "\n",
    "    def analyze_inter(self, filtered_data):\n",
    "        \"\"\"分析中介关键词\"\"\"\n",
    "        print(\"Analyzing intermediate keywords (inters)...\")\n",
    "        start_time = time.time()\n",
    "        for query in filtered_data:\n",
    "            for word in query:\n",
    "                if word != self.keyword:\n",
    "                    self.dict_add(self.inters, word)\n",
    "        inters_total = len(self.inters)\n",
    "        # 过滤出现次数较少的中介关键词\n",
    "        keys_to_delete = [key for key, count in self.inters.items() if count < self.threshold]\n",
    "        for key in keys_to_delete:\n",
    "            del self.inters[key]\n",
    "        self.num_inters = len(self.inters)\n",
    "        end_time = time.time()\n",
    "        print(f\"Got {self.num_inters}/{inters_total} inters after filtering. Time taken: {end_time - start_time:.2f}s\")\n",
    "\n",
    "    def analyze_comp(self, data, word_to_queries):\n",
    "        \"\"\"分析竞争关键词\"\"\"\n",
    "        print(\"Analyzing competing keywords (comps)...\")\n",
    "        start_time = time.time()\n",
    "        with tqdm(total=self.num_inters, desc=\"Progress\", unit=\"keyword\") as pbar:\n",
    "            for step, key in enumerate(self.inters):\n",
    "                query_start_time = time.time()\n",
    "                # 获取包含中介关键词的查询索引\n",
    "                inter_query_indices = word_to_queries.get(key, set())\n",
    "                self.inters_all[key] = len(inter_query_indices)\n",
    "                for i in inter_query_indices:\n",
    "                    query = data[i]\n",
    "                    for word in query:\n",
    "                        if word != key and word != self.keyword:\n",
    "                            self.dict_add(self.comps, (word, key))\n",
    "                query_end_time = time.time()\n",
    "                # 更新进度条\n",
    "                pbar.set_postfix(time_per_keyword=f\"{query_end_time - query_start_time:.2f}s\")\n",
    "                pbar.update(1)  # 每处理一个关键词，更新一次进度条\n",
    "        end_time = time.time()\n",
    "        print(f\"Completed analyzing comps. Total comp-inter pairs: {len(self.comps)}. Time taken: {end_time - start_time:.2f}s\")\n",
    "\n",
    "    def calculate_comp(self):\n",
    "        \"\"\"计算竞争关键词的竞争度\"\"\"\n",
    "        print(\"Calculating competition scores...\")\n",
    "        start_time = time.time()\n",
    "        for comp, inter in self.comps:\n",
    "            num = self.comps[(comp, inter)]\n",
    "            value = self.result.get(comp, 0)\n",
    "            # 跳过没有竞争关键词的中介关键词\n",
    "            if self.inters_all[inter] == self.inters[inter]:\n",
    "                continue  # 跳过该中介关键词\n",
    "\n",
    "            # 计算竞争度分数\n",
    "            self.result[comp] = value + (self.inters[inter] / self.num_queries) * (num / (self.inters_all[inter] - self.inters[inter]))\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Completed calculation of competition scores. Time taken: {end_time - start_time:.2f}s\")\n",
    "        return sorted(self.result.items(), key=lambda d: d[1], reverse=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def dict_add(_dict, key):\n",
    "        \"\"\"增加字典中 key 的计数\"\"\"\n",
    "        if key in _dict:\n",
    "            _dict[key] += 1\n",
    "        else:\n",
    "            _dict[key] = 1"
   ],
   "id": "ef006f495ce3a48e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "el相关方法定义",
   "id": "7fb68ce0ff13e0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 测试读取\n",
   "id": "adbcb45803f69f1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T15:38:17.634159Z",
     "start_time": "2024-10-29T15:38:07.140652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "# 从 CSV 文件读取数据\n",
    "with open('processed_data.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    data = [row for row in reader]"
   ],
   "id": "6a4c7ea2dcf7e52f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T15:44:07.426196Z",
     "start_time": "2024-10-29T15:44:07.378681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class CompKeyG:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        # 初始化字典和变量\n",
    "        self.name = name\n",
    "        self.inters = {}         # 中介关键词字典\n",
    "        self.comps = {}          # 竞争关键词字典\n",
    "        self.inters_all = {}     # 每个中介关键词出现的总次数\n",
    "        self.result = {}         # 存放最终竞争度排名结果\n",
    "        self.keyword = ''        # 当前处理的种子关键词\n",
    "        self.num_queries = 0     # 当前种子关键词的查询数量\n",
    "        self.num_inters = 0      # 中介关键词数量\n",
    "        self.threshold = 0       # 中介关键词过滤阈值\n",
    "        self.comp_cache = {}     # 缓存关键词竞争度排名结果\n",
    "\n",
    "        # 图结构\n",
    "        self.word_counts = {}    # 每个词在所有查询中出现的次数\n",
    "        self.co_occurrence = {}  # 词与词之间的共现次数\n",
    "\n",
    "    def initialize(self, keyword):\n",
    "        \"\"\"初始化关键词相关数据\"\"\"\n",
    "        self.inters.clear()\n",
    "        self.comps.clear()\n",
    "        self.inters_all.clear()\n",
    "        self.result.clear()\n",
    "        self.num_queries = 0\n",
    "        self.num_inters = 0\n",
    "        self.keyword = keyword\n",
    "\n",
    "    def build_graph(self, data):\n",
    "        \"\"\"构建词共现图\"\"\"\n",
    "        print(\"Building word co-occurrence graph...\")\n",
    "        start_time = time.time()\n",
    "        word_counts = defaultdict(int)\n",
    "        co_occurrence = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        for query in data:\n",
    "            unique_words = set(query)\n",
    "            # 更新每个词的出现次数\n",
    "            for word in unique_words:\n",
    "                word_counts[word] += 1\n",
    "            # 更新词之间的共现次数\n",
    "            for word1 in unique_words:\n",
    "                for word2 in unique_words:\n",
    "                    if word1 != word2:\n",
    "                        co_occurrence[word1][word2] += 1\n",
    "\n",
    "        self.word_counts = dict(word_counts)\n",
    "        self.co_occurrence = {k: dict(v) for k, v in co_occurrence.items()}\n",
    "        end_time = time.time()\n",
    "        print(f\"Graph built. Time taken: {end_time - start_time:.2f}s\")\n",
    "\n",
    "    def save_graph(self, filename=\"graph.pkl\"):\n",
    "        \"\"\"将图结构保存到磁盘\"\"\"\n",
    "        print(f\"Saving graph to '{filename}'...\")\n",
    "        try:\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump({'word_counts': self.word_counts, 'co_occurrence': self.co_occurrence}, f)\n",
    "            print(\"Graph saved successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving the graph: {e}\")\n",
    "\n",
    "    def load_graph(self, filename=\"graph.pkl\"):\n",
    "        \"\"\"从磁盘加载图结构\"\"\"\n",
    "        print(f\"Loading graph from '{filename}'...\")\n",
    "        try:\n",
    "            with open(filename, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                self.word_counts = data['word_counts']\n",
    "                self.co_occurrence = data['co_occurrence']\n",
    "            print(\"Graph loaded successfully.\")\n",
    "            # 统计节点数量\n",
    "            num_nodes = len(self.word_counts)\n",
    "            \n",
    "            # 统计边的数量\n",
    "            num_edges = sum(len(neighbors) for neighbors in self.co_occurrence.values())\n",
    "    \n",
    "            print(\"Graph loaded successfully.\")\n",
    "            print(f\"Graph has {num_nodes} nodes and {num_edges} edges.\")\n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Graph file '{filename}' not found.\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while loading the graph: {e}\")\n",
    "            return False\n",
    "\n",
    "    def train(self, data, keywords: list, num_results: int = 20, graph_filename=\"graph.pkl\"):\n",
    "        \"\"\"训练模型，对每个关键词进行竞争性分析\"\"\"\n",
    "        # 尝试从磁盘加载图结构\n",
    "        graph_loaded = self.load_graph(graph_filename)\n",
    "        if not graph_loaded:\n",
    "            # 构建词共现图\n",
    "            self.build_graph(data)\n",
    "            # 将图结构保存到磁盘\n",
    "            self.save_graph(graph_filename)\n",
    "\n",
    "        for step, keyword in enumerate(keywords):\n",
    "            print(f\"Training keyword {step + 1}/{len(keywords)}: '{keyword}'\")\n",
    "            # 初始化\n",
    "            self.initialize(keyword)\n",
    "            # 筛选包含种子关键词的查询数量\n",
    "            self.num_queries = self.word_counts.get(keyword, 0)\n",
    "            if self.num_queries == 0:\n",
    "                print(f\"No queries found containing keyword '{keyword}'. Skipping...\")\n",
    "                continue\n",
    "            # 分析中介关键词和竞争关键词\n",
    "            self.analyze_inter()\n",
    "            self.analyze_comp()\n",
    "            # 计算竞争度并缓存结果\n",
    "            self.comp_cache[keyword] = self.calculate_comp()[:num_results]\n",
    "            print(f\"Completed training for '{keyword}'\")\n",
    "\n",
    "    def save_to_disk(self, filename=\"comp_cache.json\"):\n",
    "        \"\"\"将训练结果保存到磁盘上的文件中\"\"\"\n",
    "        try:\n",
    "            # 打开文件并以 JSON 格式写入 comp_cache\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.comp_cache, f, ensure_ascii=False, indent=4)\n",
    "            print(f\"Training results saved successfully to '{filename}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving to disk: {e}\")\n",
    "\n",
    "    def predict(self, keyword):\n",
    "        \"\"\"预测指定关键词的竞争度排名\"\"\"\n",
    "        return self.comp_cache.get(keyword, None)\n",
    "\n",
    "    def analyze_inter(self):\n",
    "        \"\"\"分析中介关键词\"\"\"\n",
    "        print(\"Analyzing intermediate keywords (inters)...\")\n",
    "        start_time = time.time()\n",
    "        # 获取与种子关键词共现的所有词汇及其共现次数\n",
    "        inters = self.co_occurrence.get(self.keyword, {})\n",
    "        # 过滤出现次数较少的中介关键词\n",
    "        self.inters = {word: count for word, count in inters.items() if count >= self.threshold}\n",
    "        self.num_inters = len(self.inters)\n",
    "        end_time = time.time()\n",
    "        print(f\"Got {self.num_inters}/{len(inters)} inters after filtering. Time taken: {end_time - start_time:.2f}s\")\n",
    "\n",
    "    def analyze_comp(self):\n",
    "        \"\"\"分析竞争关键词\"\"\"\n",
    "        print(\"Analyzing competing keywords (comps)...\")\n",
    "        start_time = time.time()\n",
    "        with tqdm(total=self.num_inters, desc=\"Progress\", unit=\"keyword\") as pbar:\n",
    "            for inter in self.inters:\n",
    "                # 获取中介关键词的总出现次数\n",
    "                self.inters_all[inter] = self.word_counts.get(inter, 0)\n",
    "                # 获取与中介关键词共现的词汇及其共现次数\n",
    "                comps = self.co_occurrence.get(inter, {})\n",
    "                for comp, count in comps.items():\n",
    "                    if comp != self.keyword:\n",
    "                        key = (comp, inter)\n",
    "                        self.comps[key] = self.comps.get(key, 0) + count\n",
    "                pbar.update(1)\n",
    "        end_time = time.time()\n",
    "        print(f\"Completed analyzing comps. Total comp-inter pairs: {len(self.comps)}. Time taken: {end_time - start_time:.2f}s\")\n",
    "\n",
    "    def calculate_comp(self):\n",
    "        \"\"\"计算竞争关键词的竞争度\"\"\"\n",
    "        print(\"Calculating competition scores...\")\n",
    "        start_time = time.time()\n",
    "        for (comp, inter), num in self.comps.items():\n",
    "            value = self.result.get(comp, 0)\n",
    "            # 跳过没有竞争关键词的中介关键词\n",
    "            if self.inters_all[inter] == self.inters[inter]:\n",
    "                continue  # 跳过该中介关键词\n",
    "\n",
    "            # 计算竞争度分数\n",
    "            self.result[comp] = value + (self.inters[inter] / self.num_queries) * (num / (self.inters_all[inter] - self.inters[inter]))\n",
    "        end_time = time.time()\n",
    "        print(f\"Completed calculation of competition scores. Time taken: {end_time - start_time:.2f}s\")\n",
    "        return sorted(self.result.items(), key=lambda d: d[1], reverse=True)"
   ],
   "id": "7303b8836e72c984",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 运行模型",
   "id": "5b7a1931cab75ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T00:07:43.463953Z",
     "start_time": "2024-10-30T00:07:02.047581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义要分析的关键词列表\n",
    "keywords = [\"苹果\"]\n",
    "\n",
    "# 创建 CompKey 实例，指定模型名称\n",
    "comp_key_model = CompKeyG(name=\"KeywordCompetitionModel\")\n",
    "\n",
    "# 调用 train 方法进行训练，每个关键词返回前 10 个竞争关键词\n",
    "comp_key_model.train(data=data, keywords=keywords, num_results=10,graph_filename=\"old_graph.pkl\")\n",
    "comp_key_model.save_to_disk(\"comp_cache.json\")\n",
    "\n",
    "# 对某个关键词预测竞争度排名结果\n",
    "keyword_to_predict = \"苹果\"\n",
    "competition_results = comp_key_model.predict(keyword_to_predict)\n",
    "\n",
    "# 输出预测结果\n",
    "if competition_results:\n",
    "    print(f\"竞争关键词排名结果 for '{keyword_to_predict}':\")\n",
    "    for rank, (comp_word, score) in enumerate(competition_results, 1):\n",
    "        print(f\"{rank}. {comp_word}: {score}\")\n",
    "else:\n",
    "    print(f\"No competition data found for keyword: {keyword_to_predict}\")"
   ],
   "id": "4caf81e150902a42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph from 'old_graph.pkl'...\n",
      "Graph loaded successfully.\n",
      "Graph loaded successfully.\n",
      "Graph has 835458 nodes and 42071568 edges.\n",
      "Training keyword 1/1: '苹果'\n",
      "Analyzing intermediate keywords (inters)...\n",
      "Got 11191/11191 inters after filtering. Time taken: 0.00s\n",
      "Analyzing competing keywords (comps)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 11191/11191 [00:14<00:00, 796.20keyword/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed analyzing comps. Total comp-inter pairs: 19093854. Time taken: 14.06s\n",
      "Calculating competition scores...\n",
      "Completed calculation of competition scores. Time taken: 10.12s\n",
      "Completed training for '苹果'\n",
      "Training results saved successfully to 'comp_cache.json'\n",
      "竞争关键词排名结果 for '苹果':\n",
      "1. 的: 0.5793444461260963\n",
      "2. 怎么: 0.29552916311292643\n",
      "3. 什么: 0.26697918611865296\n",
      "4. 吗: 0.2512370447773059\n",
      "5. 手机: 0.20852050250510326\n",
      "6. 多少: 0.2067247086753324\n",
      "7. 是: 0.20130953353996792\n",
      "8.  : 0.17544105076919073\n",
      "9. 和: 0.1600091529151374\n",
      "10. 了: 0.1536104244394003\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 下面是尝试过的用处不大的代码",
   "id": "32f27a7c42c5afb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "# 初始化 Elasticsearch 连接\n",
    "es = Elasticsearch('http://localhost:9200',request_timeout=60 ) # 设置超时时间为 60 秒)\n",
    "index_name = 'queries_index'\n",
    "\n",
    "def initialize_es_index():\n",
    "    \"\"\"初始化 Elasticsearch 索引，定义映射\"\"\"\n",
    "    mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"analyzer\": \"standard\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    if not es.indices.exists(index=index_name):\n",
    "        es.indices.create(index=index_name, body=mapping)\n",
    "\n",
    "def index_data_to_es(data):\n",
    "    \"\"\"将数据批量索引到 Elasticsearch\"\"\"\n",
    "    actions = [\n",
    "        {\n",
    "            \"_index\": index_name,\n",
    "            \"_source\": {\n",
    "                \"query\": ' '.join(query)  # 将关键词列表转换为字符串\n",
    "            }\n",
    "        }\n",
    "        for query in data\n",
    "    ]\n",
    "    bulk(es, actions)\n",
    "    print(\"Data indexing completed.\")\n",
    "\n",
    "def search_queries_by_keyword_es(keyword):\n",
    "    \"\"\"在 Elasticsearch 中搜索包含指定关键词的查询\"\"\"\n",
    "    body = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"query\": keyword\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"query\"]  # 仅获取查询字段\n",
    "    }\n",
    "    # 使用 scroll API 获取所有匹配的文档\n",
    "    results = []\n",
    "    page = es.search(index=index_name, body=body, scroll='2m', size=1000)\n",
    "    sid = page['_scroll_id']\n",
    "    scroll_size = len(page['hits']['hits'])\n",
    "    while scroll_size > 0:\n",
    "        hits = page['hits']['hits']\n",
    "        for hit in hits:\n",
    "            query = hit['_source']['query'].split()\n",
    "            results.append(query)\n",
    "        page = es.scroll(scroll_id=sid, scroll='2m')\n",
    "        sid = page['_scroll_id']\n",
    "        scroll_size = len(page['hits']['hits'])\n",
    "    return results"
   ],
   "id": "a02b0d27b15145e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "class CompKeyEL:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        # 初始化字典和变量\n",
    "        self.name = name\n",
    "        self.inters = {}         # 中介关键词字典\n",
    "        self.comps = {}          # 竞争关键词字典\n",
    "        self.inters_all = {}     # 每个中介关键词出现的总次数\n",
    "        self.result = {}         # 存放最终竞争度排名结果\n",
    "        self.keyword = ''        # 当前处理的种子关键词\n",
    "        self.num_queries = 0     # 当前种子关键词的查询数量\n",
    "        self.num_inters = 0      # 中介关键词数量\n",
    "        self.threshold = 0       # 中介关键词过滤阈值\n",
    "        self.comp_cache = {}     # 缓存关键词竞争度排名结果\n",
    "    \n",
    "    def initialize(self, keyword):\n",
    "        \"\"\"初始化关键词相关数据\"\"\"\n",
    "        self.inters.clear()\n",
    "        self.comps.clear()\n",
    "        self.inters_all.clear()\n",
    "        self.result.clear()\n",
    "        self.num_queries = 0\n",
    "        self.num_inters = 0\n",
    "        self.keyword = keyword\n",
    "\n",
    "    def train(self, data, keywords: list, num_results: int = 20):\n",
    "        \"\"\"训练模型，对每个关键词进行竞争性分析\"\"\"\n",
    "        # 初始化 Elasticsearch 索引，并将数据索引到 Elasticsearch\n",
    "        print(\"Indexing data into Elasticsearch...\")\n",
    "        for step, keyword in enumerate(keywords):\n",
    "            print(f\"Training keyword {step + 1}/{len(keywords)}: '{keyword}'\")\n",
    "            # 初始化\n",
    "            self.initialize(keyword)\n",
    "            # 使用 Elasticsearch 查询包含种子关键词的查询\n",
    "            filtered_data = search_queries_by_keyword_es(keyword)\n",
    "            self.num_queries = len(filtered_data)\n",
    "            # 分析中介关键词和竞争关键词\n",
    "            self.analyze_inter(filtered_data)\n",
    "            self.analyze_comp()\n",
    "            # 计算竞争度并缓存结果\n",
    "            self.comp_cache[keyword] = self.calculate_comp()[:num_results]\n",
    "            print(f\"Completed training for '{keyword}'\")\n",
    "\n",
    "    def analyze_inter(self, filtered_data):\n",
    "        \"\"\"分析中介关键词\"\"\"\n",
    "        print(\"Analyzing intermediate keywords (inters)...\")\n",
    "        start_time = time.time()\n",
    "        for query in filtered_data:\n",
    "            for word in query:\n",
    "                if word != self.keyword:\n",
    "                    self.dict_add(self.inters, word)\n",
    "        inters_total = len(self.inters)\n",
    "        # 过滤出现次数较少的中介关键词\n",
    "        keys_to_delete = [key for key, count in self.inters.items() if count < self.threshold]\n",
    "        for key in keys_to_delete:\n",
    "            del self.inters[key]\n",
    "        self.num_inters = len(self.inters)\n",
    "        end_time = time.time()\n",
    "        print(f\"Got {self.num_inters}/{inters_total} inters after filtering. Time taken: {end_time - start_time:.2f}s\")\n",
    "\n",
    "    def analyze_comp(self):\n",
    "        \"\"\"分析竞争关键词\"\"\"\n",
    "        print(\"Analyzing competing keywords (comps)...\")\n",
    "        start_time = time.time()\n",
    "        with tqdm(total=self.num_inters, desc=\"Progress\", unit=\"keyword\") as pbar:\n",
    "            for step, key in enumerate(self.inters):\n",
    "                query_start_time = time.time()\n",
    "                # 使用 Elasticsearch 查询包含中介关键词的查询\n",
    "                inter_queries = search_queries_by_keyword_es(key)\n",
    "                self.inters_all[key] = len(inter_queries)\n",
    "                for query in inter_queries:\n",
    "                    for word in query:\n",
    "                        if word != key and word != self.keyword:\n",
    "                            self.dict_add(self.comps, (word, key))\n",
    "                query_end_time = time.time()\n",
    "                # 更新进度条\n",
    "                pbar.set_postfix(time_per_keyword=f\"{query_end_time - query_start_time:.2f}s\")\n",
    "                pbar.update(1)\n",
    "        end_time = time.time()\n",
    "        print(f\"Completed analyzing comps. Total comp-inter pairs: {len(self.comps)}. Time taken: {end_time - start_time:.2f}s\")\n",
    "\n",
    "    def calculate_comp(self):\n",
    "        \"\"\"计算竞争关键词的竞争度\"\"\"\n",
    "        print(\"Calculating competition scores...\")\n",
    "        start_time = time.time()\n",
    "        for comp, inter in self.comps:\n",
    "            num = self.comps[(comp, inter)]\n",
    "            value = self.result.get(comp, 0)\n",
    "            # 跳过没有竞争关键词的中介关键词\n",
    "            if self.inters_all[inter] == self.inters[inter]:\n",
    "                continue  # 跳过该中介关键词\n",
    "\n",
    "            # 计算竞争度分数\n",
    "            self.result[comp] = value + (self.inters[inter] / self.num_queries) * (num / (self.inters_all[inter] - self.inters[inter]))\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Completed calculation of competition scores. Time taken: {end_time - start_time:.2f}s\")\n",
    "        return sorted(self.result.items(), key=lambda d: d[1], reverse=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def dict_add(_dict, key):\n",
    "        \"\"\"增加字典中 key 的计数\"\"\"\n",
    "        if key in _dict:\n",
    "            _dict[key] += 1\n",
    "        else:\n",
    "            _dict[key] = 1"
   ],
   "id": "8f6f6183f633d07a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "initialize_es_index()\n",
    "index_data_to_es(data)"
   ],
   "id": "c383a52cd579cf81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import Counter\n",
    "# 统计词频\n",
    "word_counts = Counter(word for query in data for word in query)\n",
    "\n",
    "# 获取频率中等的10个词\n",
    "# 首先排序，然后取中间10个\n",
    "sorted_words = sorted(word_counts.items(), key=lambda item: item[1])\n",
    "middle_index = len(sorted_words) // 2\n",
    "middle_10_words = sorted_words[middle_index - 5: middle_index + 5]  # 取中间10个\n",
    "top_20_words = sorted_words[-200:-100]  # 取频率最高的10个\n",
    "bottom_10_words = sorted_words[:10]  # 取频率最低的10个\n",
    "\n",
    "# 输出结果\n",
    "print(\"Top 10 Words (Highest Frequency):\")\n",
    "for word, freq in top_20_words:\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n",
    "# print(\"\\nBottom 10 Words (Lowest Frequency):\")\n",
    "# for word, freq in bottom_10_words:\n",
    "#     print(f\"{word}: {freq}\")\n",
    "# \n",
    "# print(\"\\nMiddle 10 Words (Medium Frequency):\")\n",
    "# for word, freq in middle_10_words:\n",
    "#     print(f\"{word}: {freq}\")\n",
    "# "
   ],
   "id": "95c81077a5aa8e46"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
